{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8msAr-8IMoiN"
   },
   "source": [
    "## **Tokenization Techniques**\n",
    "\n",
    "The choice of tokenization method depends on the nature of the text data and the specific requirements of the NLP task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52ektzNUMomH"
   },
   "source": [
    "### **Whitespace Tokenization:**\n",
    "\n",
    "Method: The simplest form of tokenization involves splitting a text into tokens based on whitespace (spaces, tabs, and line breaks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmsFJbRBM_Ge",
    "outputId": "618d2b1f-3c49-4cc7-d6a1-0c126065ae7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Learning Tokenization with ML Archive \n",
      "Tokens: ['Learning', 'Tokenization', 'with', 'ML', 'Archive']\n"
     ]
    }
   ],
   "source": [
    "sentence =  \"Learning Tokenization with ML Archive\"\n",
    "tokens = sentence.split()\n",
    "print(f\"Sentence: {sentence} \\nTokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MwQ4r6aMopD"
   },
   "source": [
    "### **Word Tokenization:**\n",
    "Method: Breaking down a text into individual words is a common tokenization approach. Punctuation marks are usually treated as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A39dUv3CNsEL",
    "outputId": "558c29f8-c930-43a1-b726-d39434df7d9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Word tokenization, where Punctuation marks are treated as separate tokens.\n",
      "Word Tokens: ['Word', 'tokenization', ',', 'where', 'Punctuation', 'marks', 'are', 'treated', 'as', 'separate', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"Word tokenization, where Punctuation marks are treated as separate tokens.\"\n",
    "\n",
    "# Tokenize the sentence into words based on spaces and punctuation\n",
    "tokens = re.findall(r'\\b\\w+\\b|[.,;!?]', sentence)\n",
    "\n",
    "# Print the result\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"Word Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTO5k_Y-Moyg"
   },
   "source": [
    "## Tokenization tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Opxpf0VOQPSz"
   },
   "source": [
    "Tokenization can be accomplished using various techniques, and the choice of method often depends on the specific requirements of the natural language processing (NLP) task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ3LK0X-QPVz"
   },
   "source": [
    "### NLTK Word Tokenize\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3VHl4MkQahH",
    "outputId": "42c1d48e-7130-424c-d88e-fb7157548d0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokens: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "sentence = \"NLTK is a powerful library for natural language processing.\"\n",
    "tokens_nltk = word_tokenize(sentence)\n",
    "print(\"NLTK Tokens:\", tokens_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdguYNIpMosJ"
   },
   "source": [
    "Sentence Tokenization:\n",
    "-\tMethod: Dividing a text into sentences. This is particularly useful for tasks that require understanding the context of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8NigYHIQxYo",
    "outputId": "bff0f8f7-8b37-4f95-9a43-cc7a8cb48903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Sentence tokenization is crucial. It helps in breaking text into sentences. This is an example text for demonstration purposes.\n",
      "\n",
      "Tokenized Sentences:\n",
      "Sentence 1: Sentence tokenization is crucial.\n",
      "Sentence 2: It helps in breaking text into sentences.\n",
      "Sentence 3: This is an example text for demonstration purposes.\n"
     ]
    }
   ],
   "source": [
    "# Sample text with multiple sentences\n",
    "text = \"Sentence tokenization is crucial. It helps in breaking text into sentences. This is an example text for demonstration purposes.\"\n",
    "\n",
    "\n",
    "# Tokenize the text into sentences using sent_tokenize\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Print the result\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nTokenized Sentences:\")\n",
    "for i, sentence in enumerate(sentences, start=1):\n",
    "    print(f\"Sentence {i}: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKd1vTtlQPZa"
   },
   "source": [
    "### spaCy\n",
    "spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwKgSDZmSWHt",
    "outputId": "52efce85-19de-44a6-b29d-14cc742abbed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Tokens: ['spaCy', 'provides', 'advanced', 'tokenization', 'capabilities', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"spaCy provides advanced tokenization capabilities.\")\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "print(\"spaCy Tokens:\", tokens_spacy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwgTZEjkQPb1"
   },
   "source": [
    "### Keras Tokenizer\n",
    "Keras open-source library is one of the most reliable deep learning frameworks. To perform tokenization we use: text_to_word_sequence method from the Class Keras.preprocessing.text class. The great thing about Keras is converting the alphabet in a lower case before tokenizing it, which can be quite a time-saver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gO7LG6V6TNbq",
    "outputId": "053c5d46-0765-4dca-cc3c-f187d3fb28ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Tokens: ['keras', 'is', 'a', 'high', 'level', 'neural', 'networks', 'api']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence = \"Keras is a high-level neural networks API.\"\n",
    "tokens_keras = text_to_word_sequence(sentence)\n",
    "print(\"Keras Tokens:\", tokens_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXb9QVn8QPe4"
   },
   "source": [
    "### Hugging face tokenizer\n",
    "\n",
    "The Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFQO8ihWTWKD",
    "outputId": "6ebb3815-d70f-4424-e505-55572d7377e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Tokens: ['[CLS]', 'hugging', 'face', 'transformers', 'sim', '##plify', 'nl', '##p', 'work', '##flow', '##s', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Replace 'bert-base-uncased' with the desired model name\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Hugging Face Transformers simplify NLP workflows.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens_hugging_face = tokenizer(sentence)\n",
    "print(\"Hugging Face Tokens:\", tokenizer.convert_ids_to_tokens(tokens_hugging_face['input_ids']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAfsT3yAWEq1"
   },
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQFfbHVpeFKB"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"Text processing after tokenization involves various tasks.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7QpQCfxayOl"
   },
   "source": [
    "1.   Lowercasing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rz7Fuo5wazkK"
   },
   "outputs": [],
   "source": [
    "# Lowercasing\n",
    "tokens_lower = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVLXXWPQau7X"
   },
   "source": [
    "2.   Stopword Removal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3buzIaUQa4vG"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens_lower if token not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DNg4dUva5DI"
   },
   "source": [
    "3. Lemmatization or Stemming:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXagdbR6bHqC"
   },
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBWadmombINH"
   },
   "source": [
    "4. Removing Punctuation and Special Characters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12cZyAtCcIRH"
   },
   "outputs": [],
   "source": [
    "# Removing punctuation\n",
    "cleaned_tokens = [token for token in lemmatized_tokens if token not in string.punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDcx0NcMcit1"
   },
   "source": [
    "5. Handling Numeric Tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AF7PrMDc_Ga"
   },
   "outputs": [],
   "source": [
    "# Removing Numeric Tokens\n",
    "cleaned_tokens = [token for token in cleaned_tokens if not token.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4EguUkZceoa"
   },
   "source": [
    "6. Removing HTML Tags or Markup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_2EKaDpdcC0",
    "outputId": "02e3035c-b55b-48bc-f936-faca6ff888c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original HTML Text: <p>This is <b>bold</b> and <i>italic</i> text.</p>\n",
      "Cleaned Text (without HTML tags): This is  bold  and  italic  text.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    # Use BeautifulSoup to remove HTML tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    cleaned_text = soup.get_text(separator=' ')\n",
    "    return cleaned_text\n",
    "\n",
    "# Sample HTML text\n",
    "html_text = \"<p>This is <b>bold</b> and <i>italic</i> text.</p>\"\n",
    "\n",
    "# Remove HTML tags\n",
    "cleaned_text = remove_html_tags(html_text)\n",
    "\n",
    "print(\"Original HTML Text:\", html_text)\n",
    "print(\"Cleaned Text (without HTML tags):\", cleaned_text)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
