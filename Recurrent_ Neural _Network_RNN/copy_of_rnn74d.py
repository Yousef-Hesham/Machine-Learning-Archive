# -*- coding: utf-8 -*-
"""Copy of Rnn74d.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x0lBAfi33Vp25tVHhf2aMsu42Mc2FmtN

**Import necessary Packages**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy as sp
from sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support as score
# %matplotlib inline

"""**Read the data using pandas framework with column names label and message**"""

#dataset = pd.read_csv('/content/SMSSpamCollection',sep='\t',names=['label','message'])

from google.colab import drive
drive.mount('/content/drive')

train=pd.read_csv('/content/drive/MyDrive/help/workong remote/data:run:py/datasets/drugsComTrain_raw.csv')
test=pd.read_csv('/content/drive/MyDrive/help/workong remote/data:run:py/datasets/drugsComTest_raw.csv')

dataset = pd.concat([train, test])

dataset.drop(['uniqueID'],axis =1,inplace=True)

dataset.head()

dataset['sentiment'] = dataset['rating'].apply(lambda rating : 1 if rating > 8 else 0)
dataset.head()

"""Display first five rows from the dataset

**Based on the labels, the number of ham & spam messages are counted and plotted**
"""

dataset.groupby('sentiment').describe()

dataset.sentiment.value_counts()

count_Class=pd.value_counts(dataset['sentiment'], sort= True)
count_Class.plot(kind = 'bar',color = ["green","red"])
plt.title('Bar Plot')
plt.show()

#dataset['label'] = dataset['label'].map( {'spam': 1, 'ham': 0} )

dataset.head()

"""**Import Necessary for text preprocessing & Training**

#preprocessing
"""

import nltk
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
nltk.download('wordnet')
#stop words The most common words used in a text are “the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” etc
from nltk.corpus import stopwords

stop = stopwords.words('english')


from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer

porter = PorterStemmer()

lemmatizer = WordNetLemmatizer()

from bs4 import BeautifulSoup
import re

def review_to_words(raw_review):
    # 1. Delete HTML
    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()
    # 2. Make a space
    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)
    # 3. lower letters
    words = letters_only.lower().split()
    # 5. Stopwords
    meaningful_words = [w for w in words if not w in stop]
    # 6. lemmitization
    lemmitize_words = [lemmatizer.lemmatize(w) for w in meaningful_words]
    # 7. space join words
    return( ' '.join(lemmitize_words))

dataset['review_clean'] = dataset['review'].apply(review_to_words)

dataset.head(5)

# helps in text preprocessing
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

# helps in model building
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import SimpleRNN
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Embedding
from tensorflow.keras.callbacks import EarlyStopping

# split data into train and test set
from sklearn.model_selection import train_test_split

"""**Dataset split into Train & Test**"""

X = dataset['review_clean'] .values
y = dataset['sentiment'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

"""#Text feature engineering

**1. Tokenization**

**2. Text Encoding**

**3. Padding**
"""

t = Tokenizer()
t.fit_on_texts(X_train)

encoded_train = t.texts_to_sequences(X_train)
encoded_test = t.texts_to_sequences(X_test)
print(encoded_train[0:2])

max_length = 8
padded_train = pad_sequences(encoded_train, maxlen=max_length, padding='post')
padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')
print(padded_train)

"""**Model is Built with vocabulary size as the input size.**

**Model is compiled and summary generated**
"""

# import libraries
from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Dense

# define the model
model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=max_length))
model.add(SimpleRNN(units=24, return_sequences=False))
model.add(Dense(units=16, activation='relu'))
model.add(Dense(units=1, activation='sigmoid'))

# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# summarize the model
print(model.summary())

"""**Model is trained and validated for test dataset with 50 epochs.**

**Callback is made at an early stage when the validation loss has its first minimum value.**
"""

early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)

# fit the model
model.fit(x=padded_train,
         y=y_train,
         epochs=20,
         validation_data=(padded_test, y_test), verbose=1,
         callbacks=[early_stop]
         )

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

def c_report(y_true, y_pred):
   print("Classification Report")
   print(classification_report(y_true, y_pred))
   acc_sc = accuracy_score(y_true, y_pred)
   print("Accuracy : "+ str(acc_sc))
   return acc_sc

def plot_confusion_matrix(y_true, y_pred):
   mtx = confusion_matrix(y_true, y_pred)
   sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,
               cmap="Blues", cbar=False)
   plt.ylabel('True label')
   plt.xlabel('Predicted label')

"""**Model Predicted for test dataset.**

**Classification report and Confusion Matrix generated**
"""

preds = (model.predict(padded_test) > 0.5).astype("int32")

c_report(y_test, preds)

plot_confusion_matrix(y_test, preds)