{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import required libraries"
      ],
      "metadata": {
        "id": "_j8akE7nR-kR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RxLMAtw-KE5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokenize data and build vocabulary\n",
        "The Tokenizer class is used to tokenize the text and create a vocabulary. The out-of-vocabulary token (<OOV>) is specified to handle words that are not present in the vocabulary."
      ],
      "metadata": {
        "id": "Zu0fs8nNSJud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"the quick brown fox jumps\",\n",
        "    \"over the lazy dog\",\n",
        "]\n",
        "\n",
        "\n",
        "# Tokenize and create vocabulary\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1"
      ],
      "metadata": {
        "id": "-vvXzJ_xSI-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.\tPreprocess data to generate our targets and context words in specified window\n",
        "\n",
        "\n",
        "1.   **texts_to_sequences** method of the tokenizer is used to convert the input sentences in the corpus into sequences of integers. Each unique word in the corpus is assigned a unique integer index. The resulting sequences variable is a list of lists, where each inner list represents the sequence of word indices.\n",
        "2.   For each **target word**, a context window is defined by selecting the words within a certain range around the **target word**. The **left_window** and **right_window** variables determine the boundaries of the context window. The **context words** are then extracted from the document by slicing it based on these boundaries.\n",
        "\n"
      ],
      "metadata": {
        "id": "CuCP8y-XSZVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "context_window = 2\n",
        "\n",
        "def generate_data(corpus, window_size, tokenizer):\n",
        "    sequences = tokenizer.texts_to_sequences(corpus)\n",
        "    contexts, targets= [], []\n",
        "    for doc in sequences:\n",
        "        current_index = 0\n",
        "\n",
        "        doc_len = len(doc)\n",
        "        # grab center word and its context words\n",
        "        while current_index < doc_len:\n",
        "          # target word\n",
        "          target_word = doc[current_index]\n",
        "\n",
        "          # context words in window size\n",
        "          left_window = max(0, current_index - window_size)\n",
        "          right_window= min(current_index + window_size, doc_len)\n",
        "          context_words = doc[left_window:current_index] + doc[current_index+1: right_window]\n",
        "\n",
        "          # add conext and target to our training data\n",
        "          contexts.append(context_words)\n",
        "          targets.append(target_word)\n",
        "\n",
        "\n",
        "          current_index += 1\n",
        "    contexts = pad_sequences(contexts, maxlen=context_window*2)\n",
        "\n",
        "    return np.array(contexts), np.array(targets)\n",
        "\n",
        "X_train, y_train = generate_data(corpus, context_window, tokenizer)"
      ],
      "metadata": {
        "id": "oQ69UtDt1a3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.\tDefine our model\n",
        "\n",
        "\n",
        "*   **Embedding Layer:** This layer is responsible for creating word embeddings. It takes **one-hot encoded** words as input and converts them into dense vectors of fixed size (**embedding_dim**). The input_dim is set to vocab_size, which is the size of the vocabulary, and input_length is set to **context_window*2**, the length of the padded context sequences.\n",
        "*   **GlobalAveragePooling1D Layer:** This layer calculates the average of all the embeddings in the sequence dimension. It helps reduce the dimensionality of the data before passing it to the next layer.\n",
        "*  ** Dense Layer:** This is the output layer with a number of units equal to the vocabulary size (**vocab_size**). The activation function is set to 'softmax', which is appropriate for a multi-class classification problem. It outputs a probability distribution over the vocabulary, indicating the likelihood of each word being the target word.\n"
      ],
      "metadata": {
        "id": "rQ3y-nRNR9JI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=context_window*2),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "EFHEHfWByWk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKXAba7M1K1l",
        "outputId": "d2e98e62-01ca-4808-9a5a-9648e7a11039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 100)            1000      \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 100)               0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2010 (7.85 KB)\n",
            "Trainable params: 2010 (7.85 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "batch_size = 16\n",
        "\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTl_t4YxyYn7",
        "outputId": "269b9f4f-b7ab-4232-9f9a-c6e9aba8d712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 3s 3s/step - loss: 2.3016 - accuracy: 0.1111\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2957 - accuracy: 0.3333\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2897 - accuracy: 0.4444\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2838 - accuracy: 0.4444\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2778 - accuracy: 0.5556\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2719 - accuracy: 0.6667\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2660 - accuracy: 0.7778\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2601 - accuracy: 0.7778\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.2542 - accuracy: 0.8889\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2483 - accuracy: 0.8889\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2424 - accuracy: 0.8889\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2365 - accuracy: 0.7778\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.2305 - accuracy: 0.6667\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2245 - accuracy: 0.6667\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2185 - accuracy: 0.6667\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2125 - accuracy: 0.6667\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2064 - accuracy: 0.6667\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2003 - accuracy: 0.6667\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.1942 - accuracy: 0.6667\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.1880 - accuracy: 0.6667\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.1817 - accuracy: 0.6667\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.1754 - accuracy: 0.6667\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1691 - accuracy: 0.6667\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1627 - accuracy: 0.6667\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1562 - accuracy: 0.6667\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.1496 - accuracy: 0.6667\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1430 - accuracy: 0.6667\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1363 - accuracy: 0.6667\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1295 - accuracy: 0.6667\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1227 - accuracy: 0.6667\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1157 - accuracy: 0.6667\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.1087 - accuracy: 0.6667\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1016 - accuracy: 0.6667\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0945 - accuracy: 0.6667\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.0872 - accuracy: 0.6667\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0798 - accuracy: 0.6667\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0724 - accuracy: 0.6667\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.0649 - accuracy: 0.6667\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0572 - accuracy: 0.6667\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0495 - accuracy: 0.6667\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.0417 - accuracy: 0.6667\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0338 - accuracy: 0.6667\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.0258 - accuracy: 0.6667\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0177 - accuracy: 0.6667\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.0095 - accuracy: 0.6667\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0012 - accuracy: 0.6667\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.9928 - accuracy: 0.6667\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.9843 - accuracy: 0.5556\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.9758 - accuracy: 0.5556\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.9671 - accuracy: 0.5556\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x791c7db57760>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.\tTest our Learned weights of the words\n",
        "\n",
        "\n",
        "\n",
        "*   **get_word_vector** function. This function retrieves the learned embedding vector for a given word from the **learned_embeddings **matrix.\n",
        "*   Once the target word vector is obtained, the function computes the **cosine similarity** between the target word vector and all other word vectors in the embedding space. This is done by taking the **dot product** of the **target vector** with each row of the **learned_embeddings** matrix. The result is a vector of cosine similarity scores between the target word and all other words in the vocabulary.\n",
        "*   The indices of the words with the **highest cosine similarity** scores are then identified. The np.argsort function returns the indices that would sort the distances array in **ascending order**. By taking the **last -top_n elements**, we get the indices of the top **top_n **words with the highest similarity scores.\n",
        "*   **The indices** are used to retrieve the actual words from the index_to_word dictionary, excluding the **padding token** (index 0). The result is a list of words that are most similar to the input word, based on the learned word embeddings.\n"
      ],
      "metadata": {
        "id": "GSZ4YYWBTnaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# learned embeddings\n",
        "learned_embeddings = model.layers[0].get_weights()[0]\n",
        "\n",
        "index_to_word = {i: w for w, i in word_index.items()}\n",
        "\n",
        "def get_word_vector(word):\n",
        "    index = word_index.get(word, word_index['<OOV>'])\n",
        "    return learned_embeddings[index]\n",
        "\n",
        "word = 'fox'\n",
        "# Example: Get the word vector for the word 'bank'\n",
        "word_vector = get_word_vector(word)\n",
        "\n",
        "# Find similar words to a given word\n",
        "def find_similar_words(word, top_n=2):\n",
        "    target_vector = get_word_vector(word)\n",
        "    distances = learned_embeddings @ target_vector\n",
        "    closest_indices = np.argsort(distances)[-top_n:]\n",
        "    similar_words = [index_to_word[index] for index in closest_indices if index!=0]\n",
        "    return similar_words\n",
        "\n",
        "# Example: Find similar words to 'bank'\n",
        "similar_words = find_similar_words(word, 3)\n",
        "print(f\"Similar words to {word}:\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYcQGfkU3-Eg",
        "outputId": "9b06e356-e05c-46d6-c129-8cc4d7a3c129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar words to fox: ['brown', 'quick', 'fox']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}